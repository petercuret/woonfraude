{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- [x] UPDATEN VAN DE bag_remove KOLOMMEN\n",
    "- [x] BASE NOTEBOOK UPDATEN & MASTER NOTEBOOK HIERIN AANROEPEN (PAPERMILL)\n",
    "- [x] MACHINE LEARNING WEER LATEN WERKEN\n",
    "- [x] DOCUMENTEREN NOTEBOOKS\n",
    "- [ ] DOCUMENTEREN README\n",
    "- [ ] RandomForestRegressor gebruiken ipv RandomForestClassifier (heeft andere evaluation metric nodig, of ergens instellen dat 50+% confidence == True, en 50-% confidence == False.\n",
    "- [ ] Checken of droppen van text kolommen uit zaken df, net voorafgaand aan de train/test set creation, okay is. \n",
    "- [ ] PARAMETER OPTIMALISATIE SEARCH RUNNEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjongstra/.local/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "# Load public modules.\n",
    "import os, sys\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Get the home dir and username.\n",
    "HOME = Path.home()\n",
    "USERNAME = os.path.basename(HOME)\n",
    "\n",
    "# Set codebase path for old VAO.\n",
    "CODEBASE_PATH_OLD = os.path.join(HOME, 'Documents/woonfraude/codebase/')\n",
    "sys.path.insert(1, CODEBASE_PATH_OLD)\n",
    "                \n",
    "# Set codebase path for new VAO.\n",
    "CODEBASE_PATH_NEW = os.path.join('/data', USERNAME, 'Documents/woonfraude/codebase/')\n",
    "sys.path.insert(1, CODEBASE_PATH_NEW)\n",
    "\n",
    "# Import own modules.\n",
    "from datasets import *\n",
    "from clean import *\n",
    "from extract_features import *\n",
    "from build_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global variables.\n",
    "FORCE_DOWNLOAD = False\n",
    "FORCE_DATASET_SPECIFIC_PREPROCESSING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all datasets in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Initialize dataset objects #\n",
    "##############################\n",
    "\n",
    "adresDataset = AdresDataset()\n",
    "zakenDataset = ZakenDataset()\n",
    "stadiaDataset = StadiaDataset()\n",
    "personenDataset = PersonenDataset()\n",
    "bagDataset = BagDataset()\n",
    "hotlineDataset = HotlineDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# Download data and perform dataset-specific pre-processing steps for dataset objects #\n",
    "#######################################################################################\n",
    "\n",
    "# Forces the downloading of new data.\n",
    "if FORCE_DOWNLOAD:\n",
    "    adresDataset.download(force=True)\n",
    "    zakenDataset.download(force=True)\n",
    "    stadiaDataset.download(force=True)\n",
    "    personenDataset.download(force=True)\n",
    "    bagDataset.download(force=True)\n",
    "    \n",
    "\n",
    " # Forces the dataset specific pre-processing of the downloaded data.\n",
    "if FORCE_DATASET_SPECIFIC_PREPROCESSING:\n",
    "    \n",
    "    # Adres dataset.\n",
    "    adresDataset.load('download')\n",
    "    adresDataset.extract_leegstand()\n",
    "    adresDataset.enrich_with_woning_id()\n",
    "\n",
    "    # Zaken Dataset.\n",
    "    zakenDataset.load('download')\n",
    "    zakenDataset.add_categories()\n",
    "    zakenDataset.filter_categories()  # Verwijder meldingen met categorieeen \"woningkwaliteit\" en \"afdeling vergunningen en beheer\".\n",
    "\n",
    "    # Stadia dataset.\n",
    "    stadiaDataset.load('download')\n",
    "    stadiaDataset.add_zaak_stadium_ids()\n",
    "    stadiaDataset.add_labels()\n",
    "\n",
    "    # Bag dataset.\n",
    "    bagDataset.download(force=True)\n",
    "    bagDataset.load('download')\n",
    "    bagDataset.bag_fix()\n",
    "\n",
    "    # Hotline dataset.\n",
    "    hotlineDataset.download(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 'download_leegstand_woningId' of dataset 'adres' loaded!\n",
      "Version 'download_categories_filterCategories' of dataset 'zaken' loaded!\n",
      "Version 'download_ids_labels' of dataset 'stadia' loaded!\n",
      "Version 'download' of dataset 'personen' loaded!\n",
      "Version 'download_columnFix' of dataset 'bag' loaded!\n",
      "Version 'download' of dataset 'hotline' loaded!\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "## Load datasets ##\n",
    "###################\n",
    "# Load datasets from cache (when download and pre-processing steps in previous block have been done).\n",
    "\n",
    "adresDataset.load('download_leegstand_woningId')\n",
    "zakenDataset.load('download_categories_filterCategories')\n",
    "stadiaDataset.load('download_ids_labels')\n",
    "personenDataset.load('download')\n",
    "bagDataset.load('download_columnFix')\n",
    "hotlineDataset.load('download')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and extract features from all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe \"zaken\": Dropped 0 duplicates!\n",
      "Dataframe \"zaken\": Fixed dates!\n",
      "Dataframe \"zaken\": Cleaned out 3 dates!\n",
      "Lowered strings of cols ['beh_code', 'beh_oms', 'afg_code_beh', 'afs_code', 'afs_oms', 'afg_code_afs', 'eigenaar', 'zaak_id', 'mededelingen', 'categorie'] in df zaken!\n",
      "Missing values in df zaken have been imputed!\n",
      "Missing values (using custom strategy) of cols ['categorie'] in df zaken have been imputed!\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## Clean zaken dataset ##\n",
    "#########################\n",
    "\n",
    "zakenPipeline = Pipeline(steps=[\n",
    "    ('clean', CleanTransformer(\n",
    "        id_column=zakenDataset.id_column,\n",
    "        drop_duplicates=True,\n",
    "        fix_date_columns=['begindatum','einddatum', 'wzs_update_datumtijd'],\n",
    "        clean_dates=True,\n",
    "        lower_string_columns=True,\n",
    "        impute_missing_values=True,\n",
    "        impute_missing_values_custom={'categorie': 'missing'})\n",
    "    ),\n",
    "#     ('extract', FeatureExtractionTransformer(\n",
    "#         categorical_cols_hot=['afg_code_beh', 'beh_code', 'eigenaar', 'categorie'])\n",
    "#     )\n",
    "    ])\n",
    "\n",
    "zaken = zakenPipeline.fit_transform(zakenDataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe \"stadia\": Dropped 0 duplicates!\n",
      "Dataframe \"stadia\": Fixed dates!\n",
      "Dataframe \"stadia\": Cleaned out 0 dates!\n",
      "Lowered strings of cols ['afg_co', 'sta_code', 'sta_oms', 'afg_code_stad', 'afs_code', 'afs_oms', 'afg_code_afs', 'resultaat', 'mdr_code', 'user_created', 'user_modified', 'stadia_id', 'zaak_id', 'stadium_id', 'label'] in df stadia!\n",
      "Missing values in df stadia have been imputed!\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "## Clean stadia dataset ##\n",
    "##########################\n",
    "\n",
    "stadiaPipeline = Pipeline(steps=[\n",
    "    ('clean', CleanTransformer(\n",
    "        id_column=stadiaDataset.id_column,\n",
    "        drop_duplicates=True,\n",
    "        fix_date_columns=['begindatum', 'peildatum', 'einddatum', 'date_created',\n",
    "                          'date_modified', 'wzs_update_datumtijd'],\n",
    "        clean_dates=True,\n",
    "        lower_string_columns=True,\n",
    "        impute_missing_values=True)\n",
    "    )])\n",
    "\n",
    "stadia = stadiaPipeline.fit_transform(stadiaDataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe \"personen\": Dropped 0 duplicates!\n",
      "Dataframe \"personen\": Fixed dates!\n",
      "Lowered strings of cols ['pen_type', 'gezinsverhouding', 'geslacht', 'voorletters', 'burgerlijke_staat', 'naam', 'geheim_adres', 'voorv_mnaam', 'voorv_naam', 'meisjesnaam', 'vertrekdatum_adam', 'ind_naamgebruik', 'nat_ned', 'ind_nat_ovlp', 'verblijfstatus', 'datum_einde_vblstat', 'landcode', 'user_created', 'user_modified', 'datum_begin_vblstat', 'ais_nr', 'crv_nr', 'geheim', 'in_onderzoek', 'datum_verkrijging_vreemd', 'voorletters_zdia', 'naam_zdia', 'voorv_mnaam_zdia', 'voorv_naam_zdia', 'meisjesnaam_zdia', 'nm_dia_255', 'mnm_dia_255'] in df personen!\n",
      "Missing values in df personen have been imputed!\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "## Clean personen dataset ##\n",
    "############################\n",
    "\n",
    "personenPipeline = Pipeline(steps=[\n",
    "    ('clean', CleanTransformer(\n",
    "        id_column=personenDataset.id_column,\n",
    "        drop_duplicates=True,\n",
    "        fix_date_columns=['geboortedatum'],\n",
    "        lower_string_columns=True)\n",
    "    )])\n",
    "\n",
    "personen = personenPipeline.fit_transform(personenDataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe \"bag\": Dropped 0 duplicates!\n",
      "Lowered strings of cols ['id_nummeraanduiding', 'landelijk_id_nummeraanduiding', 'huisletter_nummeraanduiding', 'huisnummer_toevoeging_nummeraanduiding', 'postcode', 'type', 'adres_nummer', 'vervallen_nummeraanduiding', 'hoofdadres', '_openbare_ruimte_naam_nummeraanduiding', 'bron_id_nummeraanduiding', 'ligplaats_id', 'openbare_ruimte_id', 'standplaats_id', 'status_id_nummeraanduiding', 'verblijfsobject_id', '_geom', 'id_ligplaats', 'landelijk_id_ligplaats', 'vervallen_ligplaats', 'geometrie_ligplaats', 'bron_id_ligplaats', 'status_id_ligplaats', 'id_standplaats', 'landelijk_id_standplaats', 'vervallen_standplaats', 'geometrie_standplaats', 'bron_id_standplaats', 'status_id_standplaats', 'id_verblijfsobject', 'landelijk_id_verblijfsobject', 'status_coordinaat_code', 'status_coordinaat_omschrijving', 'type_woonobject_code', 'type_woonobject_omschrijving', 'geometrie_verblijfsobject', '_huisletter_verblijfsobject', 'bron_id_verblijfsobject', 'eigendomsverhouding_id', 'financieringswijze_id', 'gebruik_id', 'ligging_id', 'locatie_ingang_id', 'reden_afvoer_id', 'reden_opvoer_id', 'status_id_verblijfsobject', 'toegang_id', '_gebiedsgerichtwerken_id', '_grootstedelijkgebied_id', 'buurt_id', 'document_mutatie', 'document_nummer', 'begin_geldigheid', 'einde_geldigheid'] in df bag!\n",
      "Missing values in df bag have been imputed!\n",
      "Missing values (using mode) of cols ['status_coordinaat_code'] in df bag have been imputed!\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "## Clean BAG dataset ##\n",
    "#######################\n",
    "\n",
    "bagPipeline = Pipeline(steps=[\n",
    "    ('clean', CleanTransformer(\n",
    "        id_column=bagDataset.id_column,\n",
    "        drop_duplicates=True,\n",
    "        fix_date_columns=[],\n",
    "        drop_columns = ['indicatie_geconstateerd', 'indicatie_in_onderzoek', 'woningvoorraad'],\n",
    "        lower_string_columns=True,\n",
    "        impute_missing_values=True,\n",
    "        impute_missing_values_mode=['status_coordinaat_code'],\n",
    "#         impute_missing_values_mode=['status_coordinaat_code', 'indicatie_geconstateerd',\n",
    "#                                     'indicatie_in_onderzoek', 'woningvoorraad'],\n",
    "        fillna_columns={'_huisnummer_verblijfsobject': 0,\n",
    "                         '_huisletter_verblijfsobject': 'None',\n",
    "                         '_openbare_ruimte_naam_verblijfsobject': 'None',\n",
    "                         '_huisnummer_toevoeging_verblijfsobject': 'None',\n",
    "                         'type_woonobject_omschrijving': 'None',\n",
    "                         'eigendomsverhouding_id': 'None',\n",
    "                         'financieringswijze_id': -1,\n",
    "                         'gebruik_id': -1,\n",
    "                         'reden_opvoer_id': -1,\n",
    "                         'status_id_verblijfsobject': -1,\n",
    "                         'toegang_id': 'None'})\n",
    "    ),\n",
    "#     ('extract', FeatureExtractionTransformer(\n",
    "#         categorical_cols_hot=['status_coordinaat_code', 'type_woonobject_omschrijving',\n",
    "#                               'eigendomsverhouding_id', 'financieringswijze_id',\n",
    "#                               'gebruik_id', 'ligging_id', 'reden_opvoer_id',\n",
    "#                               'status_id_nummeraanduiding', 'toegang_id'])\n",
    "#     )\n",
    "    ])\n",
    "\n",
    "bagDataset.data = bagPipeline.fit_transform(bagDataset.data)\n",
    "bag = bagDataset.data  # For easier usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe \"hotline\": Dropped 0 duplicates!\n",
      "Lowered strings of cols ['mdw_code', 'overtreding_code', 'melder_anoniem', 'melder_naam', 'melder_emailadres', 'melder_telnr', 'situatie_schets', 'user_created', 'user_modified'] in df hotline!\n",
      "Missing values in df hotline have been imputed!\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "## Clean hotline dataset ##\n",
    "###########################\n",
    "\n",
    "hotlinePipeline = Pipeline(steps=[\n",
    "    ('clean', CleanTransformer(\n",
    "        id_column=hotlineDataset.id_column,\n",
    "        drop_duplicates=True,\n",
    "        lower_string_columns=True,\n",
    "        impute_missing_values=True)\n",
    "    )])\n",
    "\n",
    "hotlineDataset.data = hotlinePipeline.fit_transform(hotlineDataset.data)\n",
    "hotline = hotlineDataset.data  # For easier usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe \"adres\": Dropped 0 duplicates!\n",
      "Dataframe \"adres\": Fixed dates!\n",
      "Lowered strings of cols ['postcode', 'sdl_code', 'brt_code', 'pvh_cd', 'pvh_omschr', 'sbw_omschr', 'sbv_omschr', 'wzs_buurtcode_os_2015', 'wzs_buurtnaam_os_2015', 'wzs_buurtcombinatiecode_os_2015', 'wzs_buurtcombinatienaam_os_2015', 'wzs_22gebiedencode_os_2015', 'wzs_22gebiedennaam_os_2015', 'wzs_rayoncode_os_2015', 'wzs_rayonnaam_os_2015', 'wzs_stadsdeelcode_os_2015', 'wzs_stadsdeelnaam_os_2015', 'wzs_alternatieve_buurtennaam_os_2015', 'wzs_alternatieve_buurtencode_os_2015', 'wzs_geom', 'wzs_wijze_verrijking_geo', 'wzs_22gebiedencode_2015', 'wzs_22gebiedennaam_2015', 'sttnaam', 'hsltr', 'toev', 'brtcombi_naam', 'sdl_naam', 'brt_naam', 'a_dam_bag', 'landelijk_bag', 'hvv_dag_tek', 'max_vestig_dtm'] in df adres!\n",
      "Missing values in df adres have been imputed!\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## Clean adres dataset ##\n",
    "#########################\n",
    "\n",
    "# Hier de extract stap weghalen? Deze past waarschijnlijk beter na het combinen v/d datasets.\n",
    "adresPipeline = Pipeline(steps=[\n",
    "    ('clean', CleanTransformer(\n",
    "        id_column=adresDataset.id_column,\n",
    "        drop_duplicates=True,\n",
    "        fix_date_columns=['hvv_dag_tek', 'max_vestig_dtm', 'wzs_update_datumtijd'],\n",
    "        lower_string_columns=True,\n",
    "        impute_missing_values=True,\n",
    "        fillna_columns={'hsnr': 0, 'sttnaam': 'None', 'hsltr': 'None', 'toev': 'None'})\n",
    "    )\n",
    "])\n",
    "#     ,\n",
    "#     ('extract', FeatureExtractionTransformer(\n",
    "#         categorical_cols_hot=['toev', 'pvh_omschr', 'sbw_omschr', 'sbv_omschr'],\n",
    "#         ))\n",
    "#     ])\n",
    "\n",
    "adresDataset.data = adresPipeline.fit_transform(adresDataset.data)\n",
    "adres = adresDataset.data  # For easier usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in df adres have been imputed!\n",
      "Saving version 'download_leegstand_woningId_bag' of dataframe 'adres'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjongstra/.local/lib/python3.6/site-packages/pandas/core/generic.py:2378: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block1_values] [items->['postcode', 'sdl_code', 'brt_code', 'pvh_cd', 'pvh_omschr', 'sbw_omschr', 'sbv_omschr', 'wzs_buurtcode_os_2015', 'wzs_buurtnaam_os_2015', 'wzs_buurtcombinatiecode_os_2015', 'wzs_buurtcombinatienaam_os_2015', 'wzs_22gebiedencode_os_2015', 'wzs_22gebiedennaam_os_2015', 'wzs_rayoncode_os_2015', 'wzs_rayonnaam_os_2015', 'wzs_stadsdeelcode_os_2015', 'wzs_stadsdeelnaam_os_2015', 'wzs_alternatieve_buurtennaam_os_2015', 'wzs_alternatieve_buurtencode_os_2015', 'wzs_geom', 'wzs_wijze_verrijking_geo', 'wzs_22gebiedencode_2015', 'wzs_22gebiedennaam_2015', 'sttnaam', 'hsltr', 'toev', 'brtcombi_naam', 'sdl_naam', 'brt_naam', 'a_dam_bag', 'landelijk_bag', 'postcode_x', 'id_nummeraanduiding', 'landelijk_id_nummeraanduiding', 'huisletter_nummeraanduiding', 'huisnummer_toevoeging_nummeraanduiding', 'type', 'adres_nummer', '_openbare_ruimte_naam_nummeraanduiding', 'bron_id_nummeraanduiding', 'ligplaats_id', 'openbare_ruimte_id', 'standplaats_id', 'status_id_nummeraanduiding', 'verblijfsobject_id', '_geom', 'id_ligplaats', 'landelijk_id_ligplaats', 'geometrie_ligplaats', 'bron_id_ligplaats', 'status_id_ligplaats', 'id_standplaats', 'landelijk_id_standplaats', 'geometrie_standplaats', 'bron_id_standplaats', 'status_id_standplaats', 'id_verblijfsobject', 'landelijk_id_verblijfsobject', 'status_coordinaat_code', 'status_coordinaat_omschrijving', 'type_woonobject_code', 'type_woonobject_omschrijving', 'geometrie_verblijfsobject', '_huisletter_verblijfsobject', 'bron_id_verblijfsobject', 'eigendomsverhouding_id', 'financieringswijze_id', 'gebruik_id', 'ligging_id', 'locatie_ingang_id', 'reden_afvoer_id', 'reden_opvoer_id', 'status_id_verblijfsobject', 'toegang_id', '_gebiedsgerichtwerken_id', '_grootstedelijkgebied_id', 'buurt_id', 'document_nummer']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adres dataset is now enriched with BAG data.\n",
      "Now looping over all address ids that have a link with one or more inhabitants...\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "Now looping over all rows in the adres dataframe in order to add person information...\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "...done!\n",
      "Saving version 'download_leegstand_woningId_bag_personen' of dataframe 'adres'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjongstra/.local/lib/python3.6/site-packages/pandas/core/generic.py:2378: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block5_values] [items->['postcode', 'sdl_code', 'brt_code', 'pvh_cd', 'pvh_omschr', 'sbw_omschr', 'sbv_omschr', 'wzs_buurtcode_os_2015', 'wzs_buurtnaam_os_2015', 'wzs_buurtcombinatiecode_os_2015', 'wzs_buurtcombinatienaam_os_2015', 'wzs_22gebiedencode_os_2015', 'wzs_22gebiedennaam_os_2015', 'wzs_rayoncode_os_2015', 'wzs_rayonnaam_os_2015', 'wzs_stadsdeelcode_os_2015', 'wzs_stadsdeelnaam_os_2015', 'wzs_alternatieve_buurtennaam_os_2015', 'wzs_alternatieve_buurtencode_os_2015', 'wzs_geom', 'wzs_wijze_verrijking_geo', 'wzs_22gebiedencode_2015', 'wzs_22gebiedennaam_2015', 'sttnaam', 'hsltr', 'toev', 'brtcombi_naam', 'sdl_naam', 'brt_naam', 'a_dam_bag', 'landelijk_bag', 'postcode_x', 'id_nummeraanduiding', 'landelijk_id_nummeraanduiding', 'huisletter_nummeraanduiding', 'huisnummer_toevoeging_nummeraanduiding', 'type', 'adres_nummer', '_openbare_ruimte_naam_nummeraanduiding', 'bron_id_nummeraanduiding', 'ligplaats_id', 'openbare_ruimte_id', 'standplaats_id', 'status_id_nummeraanduiding', 'verblijfsobject_id', '_geom', 'id_ligplaats', 'landelijk_id_ligplaats', 'geometrie_ligplaats', 'bron_id_ligplaats', 'status_id_ligplaats', 'id_standplaats', 'landelijk_id_standplaats', 'geometrie_standplaats', 'bron_id_standplaats', 'status_id_standplaats', 'id_verblijfsobject', 'landelijk_id_verblijfsobject', 'status_coordinaat_code', 'status_coordinaat_omschrijving', 'type_woonobject_code', 'type_woonobject_omschrijving', 'geometrie_verblijfsobject', '_huisletter_verblijfsobject', 'bron_id_verblijfsobject', 'eigendomsverhouding_id', 'financieringswijze_id', 'gebruik_id', 'ligging_id', 'locatie_ingang_id', 'reden_afvoer_id', 'reden_opvoer_id', 'status_id_verblijfsobject', 'toegang_id', '_gebiedsgerichtwerken_id', '_grootstedelijkgebied_id', 'buurt_id', 'document_nummer']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adres dataset is now enriched with personen data.\n",
      "Saving version 'download_leegstand_woningId_bag_personen_hotline' of dataframe 'adres'.\n",
      "The adres dataset is now enriched with hotline data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjongstra/.local/lib/python3.6/site-packages/pandas/core/computation/expressions.py:183: UserWarning: evaluating in Python space because the '+' operator is not supported by numexpr for the bool dtype, use '|' instead\n",
      "  .format(op=op_str, alt_op=unsupported[op_str]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 16079 finished cases from a total of 31051 cases.\n",
      "Saving version 'download_categories_filterCategories_finishedCases' of dataframe 'zaken'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjongstra/.local/lib/python3.6/site-packages/pandas/core/generic.py:2378: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->['afg_code_afs', 'afg_code_beh', 'afs_code', 'afs_oms', 'beh_code', 'beh_oms', 'categorie', 'eigenaar', 'mededelingen', 'zaak_id']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe \"zaken\": added column \"woonfraude\" (binary label)\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "## Enrich adres dataset ##\n",
    "##########################\n",
    "\n",
    "# Enrich the adres dataset with information from the bag, personen and hotline datasets.\n",
    "adresDataset.enrich_with_bag(bagDataset.data)\n",
    "adresDataset.enrich_with_personen_features(personenDataset.data)\n",
    "adresDataset.add_hotline_features(hotlineDataset.data)\n",
    "\n",
    "\n",
    "##########################\n",
    "## Enrich zaken dataset ##\n",
    "##########################\n",
    "\n",
    "# Only keep the finished cases in the zaken dataset (remove all unfinished cases).\n",
    "zakenDataset.keep_finished_cases(stadiaDataset.data)\n",
    "\n",
    "# Add a label to indicate woonfraude.\n",
    "zakenDataset.add_binary_label_zaken(stadiaDataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Remove implicit label columns and superfluous columns  from adres dataset ##\n",
    "###############################################################################\n",
    "\n",
    "adres_remove = [# Remove because cols do not exists when melding is received\n",
    "                    'wzs_update_datumtijd',\n",
    "                    # Remove because cols do not add extra information.\n",
    "                    'kmrs',\n",
    "                    'straatcode',\n",
    "                    'xref',\n",
    "                    'yref',\n",
    "                    'postcode',\n",
    "                    'wzs_buurtcode_os_2015',\n",
    "                    'wzs_buurtcombinatiecode_os_2015',\n",
    "                    'wzs_stadsdeelcode_os_2015',\n",
    "#                     'sttnaam',\n",
    "                    'hvv_dag_tek', # Empty column\n",
    "                    'max_vestig_dtm', # Empty column\n",
    "                    'wzs_22gebiedencode_os_2015', # Empty column\n",
    "                    'wzs_22gebiedennaam_os_2015', # Empty column\n",
    "#                     'sdl_naam',\n",
    "                    'pvh_cd',\n",
    "                    'sbv_code',\n",
    "                    'sbw_code',\n",
    "                    'wzs_wijze_verrijking_geo',\n",
    "                    'wzs_22gebiedencode_2015',\n",
    "                    'brt_naam',\n",
    "                    'wzs_buurtnaam_os_2015',\n",
    "                    'wzs_buurtcombinatienaam_os_2015',\n",
    "                    'wzs_rayonnaam_os_2015',\n",
    "                    'wzs_rayoncode_os_2015',\n",
    "                    'wzs_stadsdeelnaam_os_2015',\n",
    "                    'wzs_alternatieve_buurtennaam_os_2015',\n",
    "                    'wzs_alternatieve_buurtencode_os_2015',\n",
    "#                     'hsltr',\n",
    "                    'wzs_geom',\n",
    "                    'brt_code',\n",
    "                    'brtcombi_code',\n",
    "                    'brtcombi_naam',\n",
    "                    'sdl_code',\n",
    "                    'wzs_22gebiedennaam_2015',\n",
    "                    'wzs_id',\n",
    "                    'a_dam_bag',\n",
    "                    'landelijk_bag']\n",
    "\n",
    "bag_remove = ['einde_geldigheid',               # Only 2 entries in column.\n",
    "              'verhuurbare_eenheden',           # Only ~2k entries in column.\n",
    "              'geometrie_ligplaats',            # Needs a lot of processing before being useful.\n",
    "#               'bron_id',                      # Only 2 entries in column.\n",
    "              'bron_id_verblijfsobject',        # Only 2 entries in column.\n",
    "              'locatie_ingang_id',              # Only 2 entries in column.\n",
    "              'reden_afvoer_id',                # Only a few entries in column.\n",
    "              '_gebiedsgerichtwerken_id',       # Superfluous (gebied).\n",
    "              '_grootstedelijkgebied_id',       # Superfluous (grootstedelijkgebied).\n",
    "              'buurt_id',                       # Superfluous (buurt).\n",
    "              # ONDERSTAANDE 4 KOLOMMEN KONDEN EERDER NIET WEG IVM MATCH MET ADRES DATAFRAME.\n",
    "              # DEZE MOETEN NU WEL WEG, DAAROM WORDT NU HIER ALLES WEGGEHAALD.\n",
    "              '_openbare_ruimte_naam_nummeraanduiding',          # Superfluous (straatnaam).\n",
    "#               '_huisnummer_nummeraanduiding',                    # Superfluous (huisnummer).\n",
    "#               '_huisletter_nummeraanduiding',                    # Superfluous (huisletter).\n",
    "#               '_huisnummer_toevoeging_nummeraanduiding',         # Superfluous (huisnummer toevoeging).\n",
    "#               'vervallen',                      # Superfluous (all values in col are equal).\n",
    "              'vervallen_nummeraanduiding',\n",
    "              'vervallen_ligplaats',\n",
    "              'vervallen_standplaats',\n",
    "              'vervallen_verblijfsobject',\n",
    "              'document_mutatie',               # Not available at time of signal.\n",
    "#               'date_modified',                  # Not available at time of signal.\n",
    "              'date_modified_nummeraanduiding', # Not available at time of signal.\n",
    "              'document_nummer',                # Not needed? (Swaan?)\n",
    "              'status_coordinaat_omschrijving', # Not needed? (Swaan?)\n",
    "              'type_woonobject_code',           # Not needed? (Swaan?)\n",
    "              'id_ligplaats',                   # Not needed.\n",
    "              'landelijk_id_ligplaats',         # Not needed.\n",
    "              'id_standplaats',                 # Not needed.\n",
    "              'landelijk_id_standplaats',       # Not needed.\n",
    "              'id_verblijfsobject',             # Not needed.\n",
    "              'landelijk_id_verblijfsobject',   # Not needed.\n",
    "              ]\n",
    "\n",
    "# Remove adres_id, since this is not a feature we want our algorihtm to try and learn from.\n",
    "# adresDataset.data.drop(columns=adres_remove + bag_remove + ['adres_id'], inplace=True)\n",
    "adresDataset.data.drop(columns=adres_remove + bag_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "## Merge the adres dataset onto the zaken dataset ##\n",
    "####################################################\n",
    "\n",
    "# Merge the adres dataset onto the zaken dataset.\n",
    "zakenDataset.data = zakenDataset.data.merge(adresDataset.data, on='adres_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform  Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now extracting features from column: 'afg_code_beh'.\n",
      "Done!\n",
      "Now extracting features from column: 'beh_code'.\n",
      "Done!\n",
      "Now extracting features from column: 'eigenaar'.\n",
      "Done!\n",
      "Now extracting features from column: 'categorie'.\n",
      "Done!\n",
      "Now extracting features from column: 'toev'.\n",
      "Done!\n",
      "Now extracting features from column: 'pvh_omschr'.\n",
      "Done!\n",
      "Now extracting features from column: 'sbw_omschr'.\n",
      "Done!\n",
      "Now extracting features from column: 'sbv_omschr'.\n",
      "Done!\n",
      "Now extracting features from column: 'status_coordinaat_code'.\n",
      "Done!\n",
      "Now extracting features from column: 'type_woonobject_omschrijving'.\n",
      "Done!\n",
      "Now extracting features from column: 'eigendomsverhouding_id'.\n",
      "Done!\n",
      "Now extracting features from column: 'financieringswijze_id'.\n",
      "Done!\n",
      "Now extracting features from column: 'gebruik_id'.\n",
      "Done!\n",
      "Now extracting features from column: 'ligging_id'.\n",
      "Done!\n",
      "Now extracting features from column: 'reden_opvoer_id'.\n",
      "Done!\n",
      "Now extracting features from column: 'status_id_nummeraanduiding'.\n",
      "Done!\n",
      "Now extracting features from column: 'toegang_id'.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "## Perform feature extraction on zaken dataset ##\n",
    "#################################################\n",
    "\n",
    "categorical_col_hot_zaken = ['afg_code_beh', 'beh_code', 'eigenaar', 'categorie']\n",
    "categorical_cols_hot_adres = ['toev', 'pvh_omschr', 'sbw_omschr', 'sbv_omschr']\n",
    "categorical_cols_hot_bag = ['status_coordinaat_code', 'type_woonobject_omschrijving',\n",
    "                            'eigendomsverhouding_id', 'financieringswijze_id',\n",
    "                            'gebruik_id', 'ligging_id', 'reden_opvoer_id',\n",
    "                            'status_id_nummeraanduiding', 'toegang_id']\n",
    "\n",
    "zakenPipeline = Pipeline(steps=[\n",
    "    ('extract', FeatureExtractionTransformer(\n",
    "        categorical_cols_hot=categorical_col_hot_zaken + categorical_cols_hot_adres + categorical_cols_hot_bag,\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "zakenDataset.data = zakenPipeline.fit_transform(zakenDataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load Finalized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving version 'final' of dataframe 'zaken'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjongstra/.local/lib/python3.6/site-packages/pandas/core/generic.py:2378: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block5_values] [items->['afg_code_afs', 'afg_code_beh', 'afs_code', 'afs_oms', 'beh_code', 'beh_oms', 'categorie', 'eigenaar', 'mededelingen', 'zaak_id', 'pvh_omschr', 'sbw_omschr', 'sbv_omschr', 'sttnaam', 'hsltr', 'toev', 'sdl_naam', 'postcode_x', 'id_nummeraanduiding', 'landelijk_id_nummeraanduiding', 'huisletter_nummeraanduiding', 'huisnummer_toevoeging_nummeraanduiding', 'type', 'adres_nummer', 'bron_id_nummeraanduiding', 'ligplaats_id', 'openbare_ruimte_id', 'standplaats_id', 'status_id_nummeraanduiding', 'verblijfsobject_id', '_geom', 'bron_id_ligplaats', 'status_id_ligplaats', 'geometrie_standplaats', 'bron_id_standplaats', 'status_id_standplaats', 'status_coordinaat_code', 'type_woonobject_omschrijving', 'geometrie_verblijfsobject', '_huisletter_verblijfsobject', 'eigendomsverhouding_id', 'financieringswijze_id', 'gebruik_id', 'ligging_id', 'reden_opvoer_id', 'status_id_verblijfsobject', 'toegang_id']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Save.\n",
    "zakenDataset.version = 'final'\n",
    "zakenDataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 'final' of dataset 'zaken' loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load.\n",
    "zakenDataset = ZakenDataset()\n",
    "zakenDataset.load('final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 16079\n",
      "Percentage positives: 53.3%\n"
     ]
    }
   ],
   "source": [
    "# Show percentage of positive samples in dataset.\n",
    "print(f\"Number of entries: {len(zakenDataset.data)}\")\n",
    "print(f\"Percentage positives: {round((zakenDataset.data.woonfraude.sum() * 100) / len(zakenDataset.data.woonfraude), 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the adres_id column.\n",
    "zakenDataset.data.drop(columns=['adres_id'], inplace=True)\n",
    "\n",
    "# Remove text columns, which can't be used for training.\n",
    "zakenDataset.data.drop(columns=['afg_code_afs', 'afs_code', 'afs_oms', 'beh_oms', 'mededelingen'], inplace=True)\n",
    "\n",
    "# Only keep numeric data columns.\n",
    "zakenDataset.data = zakenDataset.data._get_numeric_data()\n",
    "\n",
    "# Remove columns containing only NaN values.\n",
    "zakenDataset.data.drop(columns=['hoofdadres', 'begin_geldigheid'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({True: 8572, False: 7507})\n",
      "Training set shape Counter({True: 7286, False: 6381})\n",
      "Testing set shape Counter({True: 1286, False: 1126})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Split up the dataset (only use numeric data!).\n",
    "X_train, X_test, y_train, y_test = split_data_train_test(zakenDataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores.\n",
    "def report(results, n_top=10):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters and distributions to sample from.\n",
    "param_dist = {\n",
    "              \"n_estimators\": sp_randint(100, 1000),\n",
    "#               \"max_features\": ['auto'],\n",
    "              \"max_features\": sp_randint(1, 100),\n",
    "              \"max_depth\": sp_randint(1, 100),\n",
    "              \"min_samples_leaf\": [1],\n",
    "              \"min_samples_split\": sp_randint(2, 5),\n",
    "              \"bootstrap\": [True, False],\n",
    "#               \"criterion\": [\"gini\", \"entropy\"],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 843.8788692951202 seconds for 4 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.763 (std: 0.007)\n",
      "Parameters: {'bootstrap': True, 'max_depth': 19, 'max_features': 66, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 903}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.762 (std: 0.010)\n",
      "Parameters: {'bootstrap': True, 'max_depth': 82, 'max_features': 79, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 614}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.757 (std: 0.006)\n",
      "Parameters: {'bootstrap': True, 'max_depth': 62, 'max_features': 40, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 250}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.757 (std: 0.007)\n",
      "Parameters: {'bootstrap': False, 'max_depth': 85, 'max_features': 33, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 926}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run randomized search.\n",
    "clf = RandomForestClassifier()\n",
    "n_iter = 4\n",
    "random_search = RandomizedSearchCV(clf,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=n_iter,\n",
    "                                   cv=5,\n",
    "                                   n_jobs=-1,\n",
    "                                   scoring='f1')\n",
    "\n",
    "start = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print results.\n",
    "print(f\"RandomizedSearchCV took {time.time() - start} seconds for {len(random_search.cv_results_['params'])} candidate parameter settings.\")\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model for later reuse in dashboard. Manually put this model in the \"data\" folder (temporary solution).\n",
    "best_random_forest_classifier_temp = random_search.best_estimator_\n",
    "pickle.dump(best_random_forest_classifier_temp, open(\"best_random_forest_classifier_temp.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
